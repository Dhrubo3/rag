{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9442512,"sourceType":"datasetVersion","datasetId":5738346},{"sourceId":9575962,"sourceType":"datasetVersion","datasetId":5837728}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-14T06:12:13.630197Z","iopub.execute_input":"2024-10-14T06:12:13.630513Z","iopub.status.idle":"2024-10-14T06:12:14.637151Z","shell.execute_reply.started":"2024-10-14T06:12:13.630477Z","shell.execute_reply":"2024-10-14T06:12:14.636124Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/raima-new-with-mistral/new 3 (1).pdf\n/kaggle/input/raima-new-with-mistral/new2 (1).pdf\n","output_type":"stream"}]},{"cell_type":"code","source":"! nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:12:14.638761Z","iopub.execute_input":"2024-10-14T06:12:14.639181Z","iopub.status.idle":"2024-10-14T06:12:15.780264Z","shell.execute_reply.started":"2024-10-14T06:12:14.639147Z","shell.execute_reply":"2024-10-14T06:12:15.779088Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Mon Oct 14 06:12:15 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   46C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   44C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -U langchain-community","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:12:15.781715Z","iopub.execute_input":"2024-10-14T06:12:15.782047Z","iopub.status.idle":"2024-10-14T06:12:33.863474Z","shell.execute_reply.started":"2024-10-14T06:12:15.782008Z","shell.execute_reply":"2024-10-14T06:12:33.862341Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.30)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.5)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.7)\nCollecting langchain<0.4.0,>=0.3.3 (from langchain-community)\n  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\nCollecting langchain-core<0.4.0,>=0.3.10 (from langchain-community)\n  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\nCollecting langsmith<0.2.0,>=0.1.125 (from langchain-community)\n  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.32.3)\nRequirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nCollecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.3->langchain-community)\n  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (2.9.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.10->langchain-community)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.4)\nCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.8.30)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (4.4.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain-community) (2.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.3->langchain-community) (2.23.4)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.0)\nDownloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\nDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\nDownloading packaging-24.1-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, requests-toolbelt, pydantic-settings, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.3 langchain-community-0.3.2 langchain-core-0.3.10 langchain-text-splitters-0.3.0 langsmith-0.1.134 packaging-24.1 pydantic-settings-2.5.2 requests-toolbelt-1.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\nfrom IPython.display import clear_output\n\n! pip install -qq -U langchain\n! pip install -qq -U tiktoken\n! pip install -qq -U pypdf\n! pip install -qq -U faiss-gpu\n\n! pip install sentence_transformers==2.2.2\n! pip install -qq -U InstructorEmbedding\n\n! pip install -qq -U transformers \n! pip install -qq -U accelerate\n! pip install -qq -U bitsandbytes\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:12:33.866109Z","iopub.execute_input":"2024-10-14T06:12:33.866462Z","iopub.status.idle":"2024-10-14T06:14:41.732898Z","shell.execute_reply.started":"2024-10-14T06:12:33.866420Z","shell.execute_reply":"2024-10-14T06:14:41.731778Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CPU times: user 1.6 s, sys: 389 ms, total: 1.98 s\nWall time: 2min 7s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport glob\nimport textwrap\nimport time\nimport gc\n\nimport langchain\n\n### loaders\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\n\n### splits\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n### prompts\nfrom langchain import PromptTemplate\n\n### vector stores\nfrom langchain_community.vectorstores import FAISS\n\n### models\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\n\n### retrievers\nfrom langchain.chains import RetrievalQA\n\nimport torch\n\nimport transformers\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:14:41.734578Z","iopub.execute_input":"2024-10-14T06:14:41.735258Z","iopub.status.idle":"2024-10-14T06:15:01.284125Z","shell.execute_reply.started":"2024-10-14T06:14:41.735205Z","shell.execute_reply":"2024-10-14T06:15:01.283144Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"CPU times: user 12 s, sys: 1.49 s, total: 13.5 s\nWall time: 19.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"print('langchain:', langchain.__version__)\nprint('torch:', torch.__version__)\nprint('transformers:', transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:01.285553Z","iopub.execute_input":"2024-10-14T06:15:01.286568Z","iopub.status.idle":"2024-10-14T06:15:01.291518Z","shell.execute_reply.started":"2024-10-14T06:15:01.286518Z","shell.execute_reply":"2024-10-14T06:15:01.290699Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"langchain: 0.3.3\ntorch: 2.4.0\ntransformers: 4.45.2\n","output_type":"stream"}]},{"cell_type":"code","source":"len(glob.glob('/kaggle/input/raima-new-with-mistral/*'))","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:01.292692Z","iopub.execute_input":"2024-10-14T06:15:01.293535Z","iopub.status.idle":"2024-10-14T06:15:01.339281Z","shell.execute_reply.started":"2024-10-14T06:15:01.293494Z","shell.execute_reply":"2024-10-14T06:15:01.338390Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"class CFG:\n    DEBUG = False\n    \n    # LLM\n    model_name = 'microsoft/Phi-3-mini-128k-instruct'\n    temperature = 0.4\n    top_p = 0.90\n    repetition_penalty = 1.15\n    max_len = 8192\n    max_new_tokens = 512\n\n    # splitting\n    split_chunk_size = 800\n    split_overlap = 400\n    \n    # embeddings\n    embeddings_model_repo = 'BAAI/bge-base-en-v1.5'\n\n    # similar passages\n    k = 6\n    \n    # paths\n    PDFs_path = '/kaggle/input/raima-new-with-mistral/'\n    Embeddings_path =  '/kaggle/input/faiss-ml-papers-st'\n    Output_folder = './ml-papers-vectordb'","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:01.340574Z","iopub.execute_input":"2024-10-14T06:15:01.340937Z","iopub.status.idle":"2024-10-14T06:15:01.346812Z","shell.execute_reply.started":"2024-10-14T06:15:01.340902Z","shell.execute_reply":"2024-10-14T06:15:01.345879Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"loader = DirectoryLoader(\n    CFG.PDFs_path,\n    glob = \"./*3215v3.pdf\" if CFG.DEBUG else \"./*.pdf\",\n    loader_cls = PyPDFLoader,\n    show_progress = True,\n    use_multithreading = True\n)\n\ndocuments = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:01.348091Z","iopub.execute_input":"2024-10-14T06:15:01.348397Z","iopub.status.idle":"2024-10-14T06:15:02.884193Z","shell.execute_reply.started":"2024-10-14T06:15:01.348366Z","shell.execute_reply":"2024-10-14T06:15:02.883293Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'We have {len(documents)} pages in total')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:02.888773Z","iopub.execute_input":"2024-10-14T06:15:02.889154Z","iopub.status.idle":"2024-10-14T06:15:02.894252Z","shell.execute_reply.started":"2024-10-14T06:15:02.889109Z","shell.execute_reply":"2024-10-14T06:15:02.893108Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"We have 15 pages in total\n","output_type":"stream"}]},{"cell_type":"code","source":"text_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = CFG.split_chunk_size,\n    chunk_overlap = CFG.split_overlap\n)\n\ntexts = text_splitter.split_documents(documents)\n\nprint(f'We have created {len(texts)} chunks from {len(documents)} pages')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:02.895409Z","iopub.execute_input":"2024-10-14T06:15:02.895757Z","iopub.status.idle":"2024-10-14T06:15:02.909054Z","shell.execute_reply.started":"2024-10-14T06:15:02.895724Z","shell.execute_reply":"2024-10-14T06:15:02.908138Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"We have created 134 chunks from 15 pages\n","output_type":"stream"}]},{"cell_type":"code","source":"if not os.path.exists(CFG.Embeddings_path + '/index.faiss'):\n    \n    print('Creating embeddings...\\n\\n')\n\n    ### download embeddings model\n    embeddings = HuggingFaceInstructEmbeddings(\n        model_name = CFG.embeddings_model_repo,\n        model_kwargs = {\"device\": \"cuda\"}\n    )\n\n    ### create embeddings and DB\n    vectordb = FAISS.from_documents(\n        documents = texts, \n        embedding = embeddings\n    )\n\n    ### persist vector database\n    vectordb.save_local(f\"{CFG.Output_folder}/faiss_index_ml_papers\") # save in output folder\n#     vectordb.save_local(f\"{CFG.Embeddings_path}/faiss_index_ml_papers\") # save in input folder\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:02.910205Z","iopub.execute_input":"2024-10-14T06:15:02.910490Z","iopub.status.idle":"2024-10-14T06:15:13.930923Z","shell.execute_reply.started":"2024-10-14T06:15:02.910460Z","shell.execute_reply":"2024-10-14T06:15:13.929951Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"embeddings = HuggingFaceInstructEmbeddings(\n    model_name = CFG.embeddings_model_repo,\n    model_kwargs = {\"device\": \"cuda\"}\n)\n\n### load vector DB embeddings\nvectordb = FAISS.load_local(\n#     CFG.Embeddings_path, # from input folder\n    CFG.Output_folder + '/faiss_index_ml_papers', # from output folder\n    embeddings,\n    allow_dangerous_deserialization = True,\n)\n\nclear_output()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:13.932285Z","iopub.execute_input":"2024-10-14T06:15:13.932672Z","iopub.status.idle":"2024-10-14T06:15:14.063584Z","shell.execute_reply.started":"2024-10-14T06:15:13.932628Z","shell.execute_reply":"2024-10-14T06:15:14.062646Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"%%time\n\n### test if vector DB was loaded correctly\nvectordb.similarity_search('scaling laws')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:14.064728Z","iopub.execute_input":"2024-10-14T06:15:14.065032Z","iopub.status.idle":"2024-10-14T06:15:14.283460Z","shell.execute_reply.started":"2024-10-14T06:15:14.065000Z","shell.execute_reply":"2024-10-14T06:15:14.282544Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"CPU times: user 136 ms, sys: 44.2 ms, total: 180 ms\nWall time: 211 ms\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': '/kaggle/input/raima-new-with-mistral/new 3 (1).pdf', 'page': 0}, page_content='This is an open access article under the CC BY -SA license.  \\n \\nCorresponding Author:  \\nAshwini V. Zadgaonkar  \\nDepartment of Information Technology  \\nShri Ramdeobaba College of Engineering and Management  \\nNagpur, 440013, India  \\nEmail: ashwinizadgaonkar24@gmail.com  \\n \\n \\n1. INTRODUCTION   \\nNowadays a lot of information is available on the internet in a structured and unstructured form \\nstored in multiple documents. This information belongs to different domai ns and needs to be analyzed and \\nprocessed to extract the desired piece of information for a particular task. M anual processing and analy sis of \\nsuch a large repository of documents demand too much efforts and it will be ve ry much time consuming also.'),\n Document(metadata={'source': '/kaggle/input/raima-new-with-mistral/new 3 (1).pdf', 'page': 3}, page_content='machine learning model. The model is built from real documents fr om SEC EDGAR  and is open source.  \\nSavelka et al [8] proposed a framework for extracting important sentences from court j udgments so that users \\nneed not refer to lengthy case documents for understanding stat utory terms. They adopted techniques like \\nmeasuring similarity among the case sentences and user queries, using the context model for sentences, query \\noptimizations, and identify novel sentences for user queries.  The proposed framework is tested on the labeled \\ndataset of 4,635 sentences for three s tatutory queries. Kumar et al. [9] worked on finding similarity among \\nthe court judgments by using IR techniques and search engine mechanism . They have compared all term'),\n Document(metadata={'source': '/kaggle/input/raima-new-with-mistral/new 3 (1).pdf', 'page': 5}, page_content='documents ready.  \\n \\n \\nREFERENCES   \\n[1] J. Ruhl, D. M. Katz, and M. J. Bommarito, “Harnes sing legal complexity,” Science , vol. 355, no. 6332,  \\npp. 1377 -1378, 2017 , doi: 10.1126/science.aag3013.  \\n[2] A. Kanapala, S. Pal, and R. Pamula, “Text summarization from legal documents : a survey,” Artificial Intelligence \\nReview , vol. 51, no. 3, pp. 371 -402, 2019, doi: 10.1007/s10462 -017-9566 -2.  \\n[3] T. Padaya chy, B. Sc holtz and J. Wesson, “An Information Extraction Model Using a Graph Database to \\nRecommend the Most Applied Case,”  2018 International Conference on Computing, Electronics & \\nCommunications Engineering (iCCECE) , 2018, pp. 89 -94, doi: 10.1109/iCCECOME.2018.865 8659 .'),\n Document(metadata={'source': '/kaggle/input/raima-new-with-mistral/new 3 (1).pdf', 'page': 6}, page_content=\"intelligence and law . ACM, 2017, pp 149 -158, doi:  10.1145/3086512.3086527.  \\n[23] M. R. S. Marques, T. Bianco, M. Roodnejad, T. Baduel  and C. Berrou, “Machine learning for explaining and \\nranking the most influential matters of law,” ICAIL '19: Proceedings of the Seventeenth International Conference \\non Artifici al Intelligence and Law , 2019, pp. 239 -243, doi:  10.1145/3322640.3326734.  \\n[24] D. Vrandečić  and M. Krötzsch, “Wikidata: a free collaborative knowledge base ,” Communications of the ACM ,  \\nvol. 57, no. 10 , pp. 78 -85, 2014 . \\n[25] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives, “DBpedia: A Nucleus for a Web of Open \\nData ,” in The Semantic Web,  Springer , Berlin, Heide lberg, 2007, vol. 4825, pp. 722 -735, doi: 10.1007/978 -3-540-\\n76298 -0_52.\")]"},"metadata":{}}]},{"cell_type":"code","source":"pip install -U bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:14.284812Z","iopub.execute_input":"2024-10-14T06:15:14.285651Z","iopub.status.idle":"2024-10-14T06:15:26.867627Z","shell.execute_reply.started":"2024-10-14T06:15:14.285603Z","shell.execute_reply":"2024-10-14T06:15:26.866355Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.44.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"def build_model(model_repo = CFG.model_name):\n\n    print('\\nDownloading model: ', model_repo, '\\n\\n')\n\n    ### tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(model_repo)\n\n    ### quantization\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit = True,\n        bnb_4bit_quant_type = \"nf4\",\n        bnb_4bit_compute_dtype = torch.float16,\n        bnb_4bit_use_double_quant = True,\n    )        \n\n    ### model\n    model = AutoModelForCausalLM.from_pretrained(\n        model_repo,\n        quantization_config = bnb_config,\n        device_map = 'auto',\n        low_cpu_mem_usage = True,\n        trust_remote_code = True,\n    )\n\n    return tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:26.869389Z","iopub.execute_input":"2024-10-14T06:15:26.870306Z","iopub.status.idle":"2024-10-14T06:15:26.876943Z","shell.execute_reply.started":"2024-10-14T06:15:26.870254Z","shell.execute_reply":"2024-10-14T06:15:26.875831Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenizer, model = build_model(model_repo = CFG.model_name)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:15:26.878582Z","iopub.execute_input":"2024-10-14T06:15:26.879270Z","iopub.status.idle":"2024-10-14T06:16:10.614011Z","shell.execute_reply.started":"2024-10-14T06:15:26.879220Z","shell.execute_reply":"2024-10-14T06:16:10.613013Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"\nDownloading model:  microsoft/Phi-3-mini-128k-instruct \n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6875bd00283c4f96bedf7d3ca2591c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddc1a01e3b264eb6b1a7c5105dc75b0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f177a5e9e534ffa834fabd2379d6cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92810c8c85d47b08aa911b6f3aede0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6d8f98fc8c4c4d9bd4643f99245d81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974cbdab6ce047849a9948e1a4ca2c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"802494ed047f40a19957baf7936f0d0e"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a995add478f41b8a45fed464cc27dfa"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d34c8b6cbc74d9594f04028b83fc1af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a3922766dd4874b987a3ba30928322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2daee2abe984be3adb047a34d5cd0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b92774ea21904866bff9a0c279dc05f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba7dfeeec73949018cf7891c2e37bea2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef9f4c59e954cd8afd8388a4cba79fe"}},"metadata":{}}]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:10.615397Z","iopub.execute_input":"2024-10-14T06:16:10.615723Z","iopub.status.idle":"2024-10-14T06:16:10.984401Z","shell.execute_reply.started":"2024-10-14T06:16:10.615689Z","shell.execute_reply":"2024-10-14T06:16:10.983462Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"51"},"metadata":{}}]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:10.985664Z","iopub.execute_input":"2024-10-14T06:16:10.986089Z","iopub.status.idle":"2024-10-14T06:16:16.816188Z","shell.execute_reply.started":"2024-10-14T06:16:10.986023Z","shell.execute_reply":"2024-10-14T06:16:16.815132Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Phi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace=False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (qkv_proj): Linear4bit(in_features=3072, out_features=9216, bias=False)\n          (rotary_emb): Phi3LongRoPEScaledRotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"model.hf_device_map","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:16.817698Z","iopub.execute_input":"2024-10-14T06:16:16.818125Z","iopub.status.idle":"2024-10-14T06:16:16.900964Z","shell.execute_reply.started":"2024-10-14T06:16:16.818079Z","shell.execute_reply":"2024-10-14T06:16:16.899838Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'model.embed_tokens': 0,\n 'model.embed_dropout': 0,\n 'model.layers.0': 0,\n 'model.layers.1': 0,\n 'model.layers.2': 0,\n 'model.layers.3': 0,\n 'model.layers.4': 0,\n 'model.layers.5': 0,\n 'model.layers.6': 0,\n 'model.layers.7': 0,\n 'model.layers.8': 0,\n 'model.layers.9': 0,\n 'model.layers.10': 0,\n 'model.layers.11': 0,\n 'model.layers.12': 0,\n 'model.layers.13': 1,\n 'model.layers.14': 1,\n 'model.layers.15': 1,\n 'model.layers.16': 1,\n 'model.layers.17': 1,\n 'model.layers.18': 1,\n 'model.layers.19': 1,\n 'model.layers.20': 1,\n 'model.layers.21': 1,\n 'model.layers.22': 1,\n 'model.layers.23': 1,\n 'model.layers.24': 1,\n 'model.layers.25': 1,\n 'model.layers.26': 1,\n 'model.layers.27': 1,\n 'model.layers.28': 1,\n 'model.layers.29': 1,\n 'model.layers.30': 1,\n 'model.layers.31': 1,\n 'model.norm': 1,\n 'lm_head': 1}"},"metadata":{}}]},{"cell_type":"code","source":"terminators = [\n    tokenizer.eos_token_id,\n    tokenizer.bos_token_id\n]\n\n\n### hugging face pipeline\npipe = pipeline(\n    task = \"text-generation\",\n    \n    model = model,\n    \n    tokenizer = tokenizer,\n#     pad_token_id = tokenizer.eos_token_id,\n    eos_token_id = terminators,\n    \n    do_sample = True,\n#     max_length = CFG.max_len,\n    max_new_tokens = CFG.max_new_tokens,\n    \n    \n    temperature = CFG.temperature,\n    top_p = CFG.top_p,\n    repetition_penalty = CFG.repetition_penalty,\n)\n\n### langchain pipeline\nllm = HuggingFacePipeline(pipeline = pipe)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:16.902292Z","iopub.execute_input":"2024-10-14T06:16:16.902642Z","iopub.status.idle":"2024-10-14T06:16:17.466999Z","shell.execute_reply.started":"2024-10-14T06:16:16.902608Z","shell.execute_reply":"2024-10-14T06:16:17.466116Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/689143874.py:28: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n  llm = HuggingFacePipeline(pipeline = pipe)\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt_template = \"\"\"\n<|system|>\n\nYou are an expert assistant that answers questions about machine learning and Large Language Models (LLMs).\n\nYou are given some extracted parts from machine learning papers along with a question.\n\nIf you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n\nIt is very important that you ALWAYS answer the question in the same language the question is in. Remember to always do that.\n\nUse only the following pieces of context to answer the question at the end.\n\n<|end|>\n\n<|user|>\n\nContext: {context}\n\nQuestion is below. Remember to answer in the same language:\n\nQuestion: {question}\n\n<|end|>\n\n<|assistant|>\n\n\"\"\"\n\n\nPROMPT = PromptTemplate(\n    template = prompt_template, \n    input_variables = [\"context\", \"question\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:41.876134Z","iopub.execute_input":"2024-10-14T06:16:41.876525Z","iopub.status.idle":"2024-10-14T06:16:41.882327Z","shell.execute_reply.started":"2024-10-14T06:16:41.876489Z","shell.execute_reply":"2024-10-14T06:16:41.881281Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"prompt_template = \"\"\"\n<|system|>\n\nYou are an expert assistant that answers questions about machine learning and Large Language Models (LLMs).\n\nYou are given some extracted parts from machine learning papers along with a question.\n\nIf you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n\nAnswer the question **only** in English, regardless of the language of the context or question.\n\nUse only the following pieces of context to answer the question at the end.\n\n<|end|>\n\n<|user|>\n\nContext: {context}\n\nQuestion is below. Answer in English:\n\nQuestion: {question}\n\n<|end|>\n\n<|assistant|>\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:45.806912Z","iopub.execute_input":"2024-10-14T06:16:45.807324Z","iopub.status.idle":"2024-10-14T06:16:45.812282Z","shell.execute_reply.started":"2024-10-14T06:16:45.807284Z","shell.execute_reply":"2024-10-14T06:16:45.811292Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"retriever = vectordb.as_retriever(\n    search_type = \"similarity\",\n    search_kwargs = {\"k\": CFG.k}\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:49.358808Z","iopub.execute_input":"2024-10-14T06:16:49.359247Z","iopub.status.idle":"2024-10-14T06:16:49.364376Z","shell.execute_reply.started":"2024-10-14T06:16:49.359206Z","shell.execute_reply":"2024-10-14T06:16:49.363425Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"qa_chain = RetrievalQA.from_chain_type(\n    llm = llm,\n    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n    retriever = retriever, \n    chain_type_kwargs = {\"prompt\": PROMPT},\n    return_source_documents = True,\n    verbose = False\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:52.267850Z","iopub.execute_input":"2024-10-14T06:16:52.268759Z","iopub.status.idle":"2024-10-14T06:16:52.274624Z","shell.execute_reply.started":"2024-10-14T06:16:52.268715Z","shell.execute_reply":"2024-10-14T06:16:52.273721Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def wrap_text_preserve_newlines(text, width=1500):\n    # Split the input text into lines based on newline characters\n    lines = text.split('\\n')\n\n    # Wrap each line individually\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n\n    # Join the wrapped lines back together using newline characters\n    wrapped_text = '\\n'.join(wrapped_lines)\n\n    return wrapped_text\n\n\ndef process_llm_response(llm_response):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n    \n    sources_used = ' \\n'.join(\n        [\n            source.metadata['source'].split('/')[-1][:-4]\n            + ' - page: '\n            + str(source.metadata['page'])\n            for source in llm_response['source_documents']\n        ]\n    )\n    \n    ans = ans + '\\n\\nSources: \\n' + sources_used\n    \n    ### return only the text after the pattern\n    pattern = \"<|assistant|>\"\n    index = ans.find(pattern)\n    if index != -1:\n        ans = ans[index + len(pattern):]    \n    \n    return ans.strip()\n\ndef llm_ans(query):\n    start = time.time()\n    \n    llm_response = qa_chain.invoke(query)\n    ans = process_llm_response(llm_response)\n    \n    end = time.time()\n\n    time_elapsed = int(round(end - start, 0))\n    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n    return ans + time_elapsed_str","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:55.253947Z","iopub.execute_input":"2024-10-14T06:16:55.254662Z","iopub.status.idle":"2024-10-14T06:16:55.264346Z","shell.execute_reply.started":"2024-10-14T06:16:55.254619Z","shell.execute_reply":"2024-10-14T06:16:55.263466Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!pip install googletrans==4.0.0-rc1","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:59.884820Z","iopub.execute_input":"2024-10-14T06:16:59.885244Z","iopub.status.idle":"2024-10-14T06:17:15.978241Z","shell.execute_reply.started":"2024-10-14T06:16:59.885205Z","shell.execute_reply":"2024-10-14T06:17:15.977086Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting googletrans==4.0.0-rc1\n  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\nCollecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hstspreload-2024.10.1-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\nCollecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\nCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\nCollecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\nCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\nCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\nCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\nCollecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\nDownloading httpx-0.13.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\nDownloading hstspreload-2024.10.1-py3-none-any.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\nDownloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\nBuilding wheels for collected packages: googletrans\n  Building wheel for googletrans (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=83ec652ce71bd20c6a4185f8dacb1d99882d9cc8e0bc3dc8a04dc8affa75749c\n  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\nSuccessfully built googletrans\nInstalling collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n  Attempting uninstall: h11\n    Found existing installation: h11 0.14.0\n    Uninstalling h11-0.14.0:\n      Successfully uninstalled h11-0.14.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.7\n    Uninstalling idna-3.7:\n      Successfully uninstalled idna-3.7\n  Attempting uninstall: httpcore\n    Found existing installation: httpcore 1.0.5\n    Uninstalling httpcore-1.0.5:\n      Successfully uninstalled httpcore-1.0.5\n  Attempting uninstall: httpx\n    Found existing installation: httpx 0.27.0\n    Uninstalling httpx-0.27.0:\n      Successfully uninstalled httpx-0.27.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastapi 0.111.0 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlangsmith 0.1.134 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.10.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a translation pipeline for English\ntranslator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")  # Change 'es' to the detected source language if necessary\nquery = \"Tell me about Bert\"\nresult = llm_ans(query)\nclear_output()\n\n# Translate the result to English\ntranslated_result = translator(result, max_length=400)[0]['translation_text']\n\n# Print the translated result\nprint(translated_result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:17:20.837670Z","iopub.execute_input":"2024-10-14T06:17:20.838649Z","iopub.status.idle":"2024-10-14T06:18:26.165300Z","shell.execute_reply.started":"2024-10-14T06:17:20.838600Z","shell.execute_reply":"2024-10-14T06:18:26.164284Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Your input_length: 433 is bigger than 0.9 * max_length: 400. You might consider increasing your max_length manually, e.g. translator('...', max_length=400)\n","output_type":"stream"},{"name":"stdout","text":"Bert, also known as Bidirectional Encoder Representations from Transformers or BiLSTM with multilanguage encoder for the understanding of natural language is a model developed by researchers led by Jacob Devlin and his colleagues at Google AI. This model was officially introduced in the article published in Arxiv.org in 2018 under the title \"BERT: pre-training of deep bidirectional transformers for language understanding.\" The main purpose of this work was to improve the capabilities of systems based on deep neural networks to understand the semantic meaning behind words and phrases within a given sentence. To achieve this, a method called tokenization WordPiece that allows to divide complex words into smaller particles without losing their general meaning when they are processed individually by intelligent machines. In addition, these authors have provided public access both to the source code and to specialized data sets related to this technology through properly available public repositories online. Bert's effective implementation has proven to be very beneficial not only in speciflic applications of the linguistic domain but also widely extended to other areas where he can contribute to human.\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Tell me about Bert\"\nresult = llm_ans(query)\nclear_output()\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:18:26.167109Z","iopub.execute_input":"2024-10-14T06:18:26.167446Z","iopub.status.idle":"2024-10-14T06:19:16.166260Z","shell.execute_reply.started":"2024-10-14T06:18:26.167411Z","shell.execute_reply":"2024-10-14T06:19:16.165395Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Bert, también conocido como Bidirectional Encoder Representations from Transformers o BiLSTM con codificador multifrecuencia para el procesamiento del lenguaje natural en español y francés respectivamente, es un modelo desarrollado por Devlin *et al.* que se enfoca principalmente en la comprensión profunda del lenguaje humano mediante técnicas avanzadas basadas en redes neuronales. Este modelo utiliza una arquitectura bidireccional conocida como transformer biLSTM, lo cual permite analizar tanto las palabras previas como siguientes dentro de oraciones dando lugar a mejorar significativamente su capacidad para entender los matices semánticos y sintácticos presentes en el discurso lingüístico. A diferencia de otros métodos tradicionales utilizados anteriormente, Bert logra alcanzar resultados más cercanos a aquellos obtenidos por personas altamente competentes en idiomas complejos debido a sus habilidades sofisticadas de reconocimiento y interpretación de patrones lingüísticos intrincados. Además, este marco teórico propone aplicaciones potenciales específicamente diseñadas para diferentes áreas relacionadas con el tratamiento automático de grandes volúmenes de datos escritos tales como clasificación de texto, extracción de información clave, recuperación de documentos entre otras actividades críticas en campos especializados donde la precisión y rapidez son fundamentales. Sin embargo, aunque ha mostrado prominente desempeño comparativo frente a diversos sistemas existentes\nhasta la fecha, no existe evidencia directa sobre si Bert superará todos estos benchmarks sin condiciones ni limitaciones adicionales impuestas durante pruebas futuras realizadas bajo diversas circunstancias operativas distintas. La investigación continuada podría revelar nuevas maneras innovadoras de optimización o adaptabilidad necesarias para mantenerse actualizado ante cambios constantes en paradigmas tecnológicos emergentes así mismo contribuirá valioso contenido académico hacia esta rama interdisciplinar. En resumen, mientras Bert demuestra ser uno de los pioneros herramientas líderes hoy día gracias a su robustez computacional combinada con alta exactitud interpretativa\n\nSources: \nnew2 (1) - page: 5 \nnew2 (1) - page: 5 \nnew2 (1) - page: 5 \nnew2 (1) - page: 3 \nnew2 (1) - page: 3 \nnew 3 (1) - page: 3\n\nTime elapsed: 50 s\n","output_type":"stream"}]},{"cell_type":"code","source":"query = \"Bert, también conocido como?\"\nresult = llm_ans(query)\nclear_output()\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:19:16.167524Z","iopub.execute_input":"2024-10-14T06:19:16.167833Z","iopub.status.idle":"2024-10-14T06:20:05.818108Z","shell.execute_reply.started":"2024-10-14T06:19:16.167799Z","shell.execute_reply":"2024-10-14T06:20:05.817199Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Bert es conocida por su nombre en inglés, que significa 'Bert'. Sin embargo, el término utilizado para referirse específicamente al modelo pre-entrenado desarrollado por Google se llama 'BERT', siglas del título original English Transformers Pre-Trained for Natural Language Understanding.' Idem est usum qui potestur non habet ut res quaeritur. Elaborated Textbook-level Solution would involve explaining how named entities within natural languages often have names or titles associated with them which might differ across various linguistic communities due to translation nuances. For instance, while 'bert' could simply mean someone called Bert without any specific connotation outside certain cultural references, when referring specifically to technology terms especially those coined after people’s names ('Bobby'), these may carry additional weight because they directly associate human identity with technical innovation—in essence creating brand identities around products derived from individuals who were instrumental during product creation phases. However, since we must adhere strictly to factual accuracy according to provided source materials rather than speculative reasoning beyond what has been explicitly stated therein – particularly relevant here where our focus lies upon accurately reflecting terminologies related to Machine Learning systems — one should note that ‘BERT,’ being both acronymic representation standing for 'Bidirectional Encoder Representations from\nTransformers,' does indeed bear significance solely tied back to its functionary role established via scholarly pursuits led initially by Jacob Devlin among others at Google Research Inc., henceforth encapsulated purely descriptively sans attribution towards personal nomenclature unless otherwise substantiated historical evidence suggests so pertaining documentation standards observed typically foundational principles governing academia driven computational advancements discourse practices prevailingly acknowledged universally irrespective crosslinguistically amongst global techno-academic fraternity circles inclusivity fostering intellectual exchange facilitation paradigms promulgate perpetually thereby ensuring consistency uniform applicability whilst maintain fidelity integrity preservation throughout all endeavors undertaken aimed toward propagating said transformative architectural marvel inherently imbued intrinsically encoded latent potential await ing actual realisation manifest aspirational objectives ultimately culminate predestined trajectories transcendent temporal constraints boundless horizons unfurl limitlessly ahead ever forward propelling momentum sustained relentless progression persisting steadfast\n\nSources: \nnew 3 (1) - page: 3 \nnew 3 (1) - page: 0 \nnew 3 (1) - page: 3 \nnew 3 (1) - page: 7 \nnew2 (1) - page: 3 \nnew2 (1) - page: 5\n\nTime elapsed: 50 s\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:17.497398Z","iopub.status.idle":"2024-10-14T06:16:17.497764Z","shell.execute_reply.started":"2024-10-14T06:16:17.497583Z","shell.execute_reply":"2024-10-14T06:16:17.497602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"O que é Fusão de Recursos Multiclasse??\"\nresult = llm_ans(query)\nclear_output()\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:17.498962Z","iopub.status.idle":"2024-10-14T06:16:17.499317Z","shell.execute_reply.started":"2024-10-14T06:16:17.499144Z","shell.execute_reply":"2024-10-14T06:16:17.499162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Población de la base de conocimientos para el procesamiento de textos legales.\"\nresult = llm_ans(query)\nclear_output()\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:16:17.500850Z","iopub.status.idle":"2024-10-14T06:16:17.501226Z","shell.execute_reply.started":"2024-10-14T06:16:17.501026Z","shell.execute_reply":"2024-10-14T06:16:17.501044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport time\nimport textwrap\nimport gc\nimport torch\n\n### langchain imports\nfrom langchain import PromptTemplate\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain_community.vectorstores import FAISS\n\n### transformers imports\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM,\n    BitsAndBytesConfig, pipeline\n)\n\n# Configuration class\nclass CFG:\n    DEBUG = False\n    \n    # LLM\n    model_name = 'microsoft/Phi-3-mini-128k-instruct'\n    temperature = 0.4\n    top_p = 0.90\n    repetition_penalty = 1.15\n    max_len = 8192\n    max_new_tokens = 512\n\n    # splitting\n    split_chunk_size = 800\n    split_overlap = 400\n    \n    # embeddings\n    embeddings_model_repo = 'BAAI/bge-base-en-v1.5'\n\n    # similar passages\n    k = 6\n    \n    # paths\n    PDFs_path = '/kaggle/input/raima-new-with-mistral/'\n    Embeddings_path =  '/kaggle/input/faiss-ml-papers-st'\n    Output_folder = './ml-papers-vectordb'\n\n# Function to load documents from PDFs\nloader = DirectoryLoader(\n    CFG.PDFs_path,\n    glob = \"./*.pdf\",\n    loader_cls = PyPDFLoader,\n    show_progress = True,\n    use_multithreading = True\n)\n\ndocuments = loader.load()\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size = CFG.split_chunk_size,\n    chunk_overlap = CFG.split_overlap\n)\n\ntexts = text_splitter.split_documents(documents)\n\n# Loading or creating embeddings\nif not os.path.exists(CFG.Embeddings_path + '/index.faiss'):\n    embeddings = HuggingFaceInstructEmbeddings(\n        model_name = CFG.embeddings_model_repo,\n        model_kwargs = {\"device\": \"cuda\"}\n    )\n    vectordb = FAISS.from_documents(documents=texts, embedding=embeddings)\n    vectordb.save_local(f\"{CFG.Output_folder}/faiss_index_ml_papers\")\nelse:\n    vectordb = FAISS.load_local(\n        CFG.Output_folder + '/faiss_index_ml_papers',\n        embeddings,\n        allow_dangerous_deserialization=True,\n    )\n\n# Model building function\ndef build_model(model_repo=CFG.model_name):\n    print('\\nDownloading model: ', model_repo, '\\n\\n')\n\n    tokenizer = AutoTokenizer.from_pretrained(model_repo)\n\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_repo,\n        quantization_config=bnb_config,\n        device_map='auto',\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\n\n    return tokenizer, model\n\n# Create the LLM pipeline\ntokenizer, model = build_model(model_repo=CFG.model_name)\nmodel.eval()\nterminators = [tokenizer.eos_token_id, tokenizer.bos_token_id]\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    eos_token_id=terminators,\n    do_sample=True,\n    max_new_tokens=CFG.max_new_tokens,\n    temperature=CFG.temperature,\n    top_p=CFG.top_p,\n    repetition_penalty=CFG.repetition_penalty,\n)\n\nllm = HuggingFacePipeline(pipeline=pipe)\n\n# Template for the prompt\nprompt_template = \"\"\"\n<|system|>\n\nYou are an expert assistant that answers questions about machine learning and Large Language Models (LLMs).\n\nYou are given some extracted parts from machine learning papers along with a question.\n\nIf you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n\nIt is very important that you ALWAYS answer the question in the same language the question is in. Remember to always do that.\n\nUse only the following pieces of context to answer the question at the end.\n\n<|end|>\n\n<|user|>\n\nContext: {context}\n\nQuestion is below. Remember to answer in the same language:\n\nQuestion: {question}\n\n<|end|>\n\n<|assistant|>\n\"\"\"\n\nPROMPT = PromptTemplate(\n    template=prompt_template,\n    input_variables=[\"context\", \"question\"]\n)\n\nretriever = vectordb.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": CFG.k}\n)\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=retriever,\n    chain_type_kwargs={\"prompt\": PROMPT},\n    return_source_documents=True,\n    verbose=False\n)\n\n# Wrapping function for preserving newlines\ndef wrap_text_preserve_newlines(text, width=1500):\n    lines = text.split('\\n')\n    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n    wrapped_text = '\\n'.join(wrapped_lines)\n    return wrapped_text\n\n# Function to calculate accuracy (this is a placeholder and can be customized)\ndef calculate_accuracy(predicted, actual):\n    if predicted == actual:\n        return 100\n    else:\n        accuracy = len(set(predicted.split()).intersection(set(actual.split()))) / len(actual.split()) * 100\n        return round(accuracy, 2)\n\n# Processing the LLM response and appending accuracy\ndef process_llm_response(llm_response, expected_answer=\"\"):\n    ans = wrap_text_preserve_newlines(llm_response['result'])\n    sources_used = ' \\n'.join(\n        [\n            source.metadata['source'].split('/')[-1][:-4]\n            + ' - page: '\n            + str(source.metadata['page'])\n            for source in llm_response['source_documents']\n        ]\n    )\n    ans = ans + '\\n\\nSources: \\n' + sources_used\n\n    pattern = \"<|assistant|>\"\n    index = ans.find(pattern)\n    if index != -1:\n        ans = ans[index + len(pattern):]\n    \n    ans = ans.strip()\n\n    if expected_answer:\n        accuracy = calculate_accuracy(ans, expected_answer)\n        ans += f\"\\n\\nAccuracy: {accuracy}%\"\n\n    return ans\n\n# Main function to get LLM answer with accuracy\ndef llm_ans(query, expected_answer=\"\"):\n    start = time.time()\n    llm_response = qa_chain.invoke(query)\n    ans = process_llm_response(llm_response, expected_answer)\n    end = time.time()\n\n    time_elapsed = int(round(end - start, 0))\n    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n    return ans + time_elapsed_str\n\n# Example usage\nquery = \"Tell me about Bert\"\nexpected_answer = \"BERT is a transformer-based model designed by Google for natural language understanding tasks.\"\nresult = llm_ans(query, expected_answer)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T06:20:05.820537Z","iopub.execute_input":"2024-10-14T06:20:05.821105Z","iopub.status.idle":"2024-10-14T06:21:09.138226Z","shell.execute_reply.started":"2024-10-14T06:20:05.821039Z","shell.execute_reply":"2024-10-14T06:21:09.137273Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n\nDownloading model:  microsoft/Phi-3-mini-128k-instruct \n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28841752240442bdba2af91d64030769"}},"metadata":{}},{"name":"stdout","text":"Bert, también conocido como Bidirectional Encoder Representations from Transformers o BiLSTM con codificadores multilínea para el entendimiento del lenguaje natural en español, es un modelo desarrollado por investigadores liderados por Jacob Devlin y sus colegas. Este algoritmo utiliza una red neuronal profunda bidireccional basada en transformadores que tiene la capacidad de comprender el significado semántico completo de las palabras dentro del contexto dado. Esto significa que puede reconocer cómo diferentes combinaciones de palabras pueden tener distintos significados dependiendo de su posición relativa entre sí. Por ejemplo, mientras 'un banco deposita dinero', se refiere a finanzas bancarias, otra oración podría referirse a un banco río abajo utilizando este tipo de tecnología lingüística avanzada. El uso principal de Bert ha sido pre-entrenar sobre grandes cantidades de datos literarios disponibles públicamente antes de aplicarlo a varias tareas específicas relacionadas con procesamiento de lenguaje humano, tales como clasificación de documentos, extracción de información clave, recuperación de información, traducción automática y mucho más. La innovación radica no solo en los parámetros inicializados mediante técnicas estadísticas sino también en la arquitectura propia del mismo, capaz de manejar dependencias tanto hacia adelante como hacia atrás simultáneamente durante la interpretación gramatical. A pesar de estos logros sobresaldientes, existen limitaciones\ninherentes debido a factores externos, incluyendo restricciones presupuestarias, recursos computacionales necesarios para ejecutar esta herramienta así como requerimientos especializados conocidos coloquialmente como hardware dedicado. Sin embargo, estas son áreas donde persisten potenciales mejoras futuras enfocándose principalmente en optimización matemática y aprendizaje adaptativo supervisado/desupervised dirigido directamente contra problemas realistas actuales presentes en campos emergentes tal cual lo demostró recientemente cuando fue empleado exitosamente junto con otros métodos modernos de inteligencia artificial similares bajo condiciones operativas prácticamente rigurosas pero estrictamente\n\nSources: \nnew2 (1) - page: 5 \nnew2 (1) - page: 5 \nnew2 (1) - page: 5 \nnew2 (1) - page: 3 \nnew2 (1) - page: 3 \nnew 3 (1) - page: 3\n\nAccuracy: 15.38%\n\nTime elapsed: 50 s\n","output_type":"stream"}]}]}